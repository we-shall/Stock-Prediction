{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each website crawled will be saved in different folder\n",
    "def create_project_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print('Creating directory ' + directory)\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# createProjectDirectory(\"Crawler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create queue and crawled files if not created\n",
    "def create_data_files(project_name, base_url):\n",
    "    queue = os.path.join(project_name , 'queue.txt')\n",
    "    crawled = os.path.join(project_name,\"crawled.txt\")\n",
    "    if not os.path.isfile(queue):\n",
    "        write_file(queue, base_url)\n",
    "    if not os.path.isfile(crawled):\n",
    "        write_file(crawled, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data onto an existing file\n",
    "def append_to_file(path, data):\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(data + '\\n')\n",
    "\n",
    "#clear the file \n",
    "# Delete the contents of a file\n",
    "def delete_file_contents(path):\n",
    "    open(path, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now chances are that same link are occuring multiple times in a file so...\n",
    "# we will create a set which read the file one by one and each link into a set\n",
    "#rt stands for read text...\n",
    "def file_to_set(file_name):\n",
    "    results = set()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        for line in f:\n",
    "            results.add(line.replace('\\n', ''))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a set to a file \n",
    "def set_to_file(links, file_name):\n",
    "    with open(file_name,\"w\") as f:\n",
    "        for l in sorted(links):\n",
    "            f.write(l+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "class LinkFinder(HTMLParser):\n",
    "\n",
    "    def __init__(self, base_url, page_url):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.page_url = page_url\n",
    "        self.links = set()\n",
    "\n",
    "    # When we call HTMLParser feed() this function is called when it encounters an opening tag <a>\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for (attribute, value) in attrs:\n",
    "                if attribute == 'href':\n",
    "                    url = parse.urljoin(self.base_url, value)\n",
    "                    self.links.add(url)\n",
    "\n",
    "    def page_links(self):\n",
    "        return self.links\n",
    "\n",
    "    def error(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rknec.edu\n"
     ]
    }
   ],
   "source": [
    "#this block is used for crawling website related to domain name if we do not do this ...\n",
    "#the crawled will crwal whole internet so to stop this we are using network parser\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "#get sub domain name for getting link which is useful\n",
    "\n",
    "def get_domain_name(url):\n",
    "    try:\n",
    "        results = get_sub_domain_name(url).split('.')\n",
    "        return results[-2]+'.'+results[-1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_sub_domain_name(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return ''\n",
    "print (get_domain_name('http://www.rknec.edu/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets program the spider for connecting to the real server\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#now for performing task of crawling faster we'll create objects which are shared among various spiders...for\n",
    "#instance there are 6 spiders for extracting the links from the website....\n",
    "class Spider:\n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name = ''\n",
    "    queue_file = ''\n",
    "    crawled_file = ''\n",
    "    queue = set()\n",
    "    crawled = set()\n",
    "\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name = project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name = domain_name\n",
    "        Spider.queue_file = Spider.project_name + '/queue.txt'\n",
    "        Spider.crawled_file = Spider.project_name + '/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "\n",
    "    # Creates directory and files for project on first run and starts the spider\n",
    "    @staticmethod\n",
    "    def boot():\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "\n",
    "    # Updates user display, fills queue and updates files\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name + ' now crawling ' + page_url)\n",
    "            print('Queue ' + str(len(Spider.queue)) + ' | Crawled  ' + str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "\n",
    "    # Converts raw response data into readable information and checks for proper html formatting\n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        #this method is for taking a page url and then loading the contents of that url\n",
    "        #the data from the urllib will be in bianry format we need to convert it into string format \n",
    "        #so we use a decoder to convert it into string format\n",
    "        #now chances are that the website doesnot allow the user to crawl the website we need to handle it by trycatch\n",
    "        html_string = ''\n",
    "        try:\n",
    "            response = urlopen(page_url)\n",
    "            if 'text/html' in response.getheader('Content-Type'):\n",
    "                html_bytes = response.read()\n",
    "                html_string = html_bytes.decode(\"utf-8\")\n",
    "            finder = LinkFinder(Spider.base_url, page_url)\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "\n",
    "    # Saves queue data to project files\n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "            Spider.queue.add(url)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory rcoem\n",
      "First spider now crawling http://www.rknec.edu/\n",
      "Queue 1 | Crawled  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Spider at 0x10e07b0b8>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the main program which will call all spiders and various methods\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "#queue is job and threads are the spider which will do the job from queue\n",
    "#constants\n",
    "PROJECT_NAME = 'rcoem'\n",
    "HOMEPAGE = 'http://www.rknec.edu/'\n",
    "DOMAIN_NAME = get_domain_name(HOMEPAGE)\n",
    "QUEUE_FILE = PROJECT_NAME + \"/queue.txt\"\n",
    "CRAWLED_FILE = PROJECT_NAME + '/crawled.txt'\n",
    "\n",
    "NUMBER_OF_THREADS = 8\n",
    "\n",
    "queue = Queue()\n",
    "Spider(PROJECT_NAME,HOMEPAGE,DOMAIN_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (__init__.py, line 53)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-61e54eb611f6>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from bs4 import BeautifulSoup\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m971\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m951\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m894\u001b[0m, in \u001b[1;35m_find_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1157\u001b[0m, in \u001b[1;35mfind_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1131\u001b[0m, in \u001b[1;35m_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1112\u001b[0m, in \u001b[1;35m_legacy_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m441\u001b[0m, in \u001b[1;35mspec_from_loader\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap_external>\"\u001b[0;36m, line \u001b[0;32m544\u001b[0;36m, in \u001b[0;35mspec_from_file_location\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/beautifulsoup4-4.6.0-py3.6.egg/bs4/__init__.py\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    'You are trying to run the Python 2 version of Beautiful Soup under Python 3. This will not work.'<>'You need to convert the code, either by installing it (`python setup.py install`) or by running 2to3 (`2to3 -w bs4`).'\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "'''\n",
    "URL of the archive web-page which provides link to\n",
    "all video lectures. It would have been tiring to\n",
    "download each video manually.\n",
    "In this example, we first crawl the webpage to extract\n",
    "all the links and then download videos.\n",
    "'''\n",
    " \n",
    "# specify the URL of the archive here\n",
    "archive_url = \"https://www.nseindia.com/corporates/corporateHome.html\"\n",
    " \n",
    "def get_video_links():\n",
    "     \n",
    "    # create response object\n",
    "    r = requests.get(archive_url)\n",
    "     \n",
    "    # create beautiful-soup object\n",
    "    soup = BeautifulSoup(r.content,'html5lib')\n",
    "     \n",
    "    # find all links on web-page\n",
    "    links = soup.findAll('a')\n",
    " \n",
    "    # filter the link sending with .mp4\n",
    "    video_links = [archive_url + link['href'] for link in links if link['href'].endswith('pd')]\n",
    " \n",
    "    return video_links\n",
    "\n",
    "print (get_video_links())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
